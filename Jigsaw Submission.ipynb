{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsZG8HwqtgG1",
    "outputId": "9da94640-65ae-4711-ffa2-7b3a7ebd39c0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "_DyC-ZXAtgG9",
    "outputId": "2629b59a-48bf-40bf-e368-93282b51769d"
   },
   "outputs": [],
   "source": [
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcmsojX8tgHF"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "data_1 = reduce_mem_usage(pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv', header=0, sep=',', quotechar='\"'))\n",
    "data_2 = reduce_mem_usage(pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv', header=0, sep=',', quotechar='\"'))\n",
    "data_3 = reduce_mem_usage(pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/validation.csv', header=0, sep=',', quotechar='\"'))\n",
    "submission = reduce_mem_usage(pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv', header=0, sep=',', quotechar='\"'))\n",
    "test = reduce_mem_usage(pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/test.csv', header=0, sep=',', quotechar='\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdkIdK7YtgHO"
   },
   "outputs": [],
   "source": [
    "data = pd.concat((data_1[['comment_text','toxic']], data_2[['comment_text','toxic']], data_3[['comment_text','toxic']]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaUifJSmtgHV"
   },
   "outputs": [],
   "source": [
    "data_1 = None\n",
    "data_2 = None\n",
    "data_3 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsCK5RY6tgHb"
   },
   "outputs": [],
   "source": [
    "training_sentences = data['comment_text']\n",
    "training_labels = data['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39HyQaA7tgHg"
   },
   "outputs": [],
   "source": [
    "testing_sentences = test['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data;\n",
    "del test;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9HZVkfmtgHo"
   },
   "outputs": [],
   "source": [
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "    \n",
    "training_sentences = training_sentences.apply(remove_between_square_brackets)\n",
    "testing_sentences = testing_sentences.apply(remove_between_square_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wppB7b0rtgHt"
   },
   "outputs": [],
   "source": [
    "def remove_escape_sequence(text):\n",
    "  return re.sub('[\\n\\r\\t]', '',text)\n",
    "\n",
    "training_sentences = training_sentences.apply(remove_escape_sequence)\n",
    "testing_sentences = testing_sentences.apply(remove_escape_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AejM9-xUtgHy"
   },
   "outputs": [],
   "source": [
    "def _removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "training_sentences = training_sentences.apply(_removeNonAscii)\n",
    "testing_sentences = testing_sentences.apply(_removeNonAscii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgxtdKDltgH3"
   },
   "outputs": [],
   "source": [
    "def _lower(s): return \"\".join(i.lower() for i in s)\n",
    "\n",
    "training_sentences = training_sentences.apply(_lower)\n",
    "testing_sentences = testing_sentences.apply(_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNMps8FjtgH-"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(['english','turkish','spanish'])\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "training_sentences = training_sentences.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "testing_sentences = testing_sentences.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYLN3bW-tgIG"
   },
   "outputs": [],
   "source": [
    "vocab_size = 500000  \n",
    "embediing_dim = 256\n",
    "max_length = 128\n",
    "trunc_type = 'pre'\n",
    "padding_type = 'pre'\n",
    "oov_token = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRkdFTlxtgIL",
    "outputId": "9ccfd99b-d0d6-47f2-8a7b-3b03d7ec79c2"
   },
   "outputs": [],
   "source": [
    "training_labels[training_labels!=0.0] = 1\n",
    "training_labels = training_labels.astype(int)\n",
    "training_labels = np.array(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGm1DqYttgIP"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, \n",
    "                      oov_token=oov_token, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                      lower=True, \n",
    "                      split=\" \", \n",
    "                      char_level=False)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, \n",
    "                                maxlen = max_length, \n",
    "                                padding = padding_type, \n",
    "                                truncating = trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, \n",
    "                               maxlen = max_length, \n",
    "                               padding = padding_type, \n",
    "                               truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = None \n",
    "testing_sentences = None\n",
    "training_sequences = None \n",
    "testing_sequences = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del training_sentences; \n",
    "del testing_sentences; \n",
    "del training_sequences; \n",
    "del testing_sequences;\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOGZfKw4tgIW"
   },
   "outputs": [],
   "source": [
    "# Initiate model\n",
    "model = tf.keras.Sequential()\n",
    "# Add Embedding layer\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embediing_dim, input_length = max_length, trainable=True))\n",
    "# Add Convolutional layer\n",
    "model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(3))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "# Add fully connected layers\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14Y46NITtgIb",
    "outputId": "5e818bb3-4657-4260-b594-d73d056eafe1"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d22Q8eOstgIg",
    "outputId": "f1e79dfa-061a-4b05-de8c-c4de64106f7f"
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "  num_epochs = 10\n",
    "  history = model.fit(training_padded, training_labels,batch_size=5120, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7Lur-cRwv6Y"
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict(testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt3LGVZyxMOk"
   },
   "outputs": [],
   "source": [
    "submission['toxic'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ni-GoCMxoPJ"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "kernel470016c642.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
